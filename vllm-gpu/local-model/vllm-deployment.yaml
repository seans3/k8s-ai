apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-gemma-1b
  labels:
    app: vllm-gemma-1b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-gemma-1b
  template:
    metadata:
      annotations:
        gke-gcs-fuse-csi: "true"
      labels:
        app: vllm-gemma-1b
        ai.gke.io/model: gemma-3-1b-it
        ai.gke.io/inference-server: vllm
    spec:
      serviceAccountName: vllm-sa
      containers:
      - name: vllm-container
        image: vllm/vllm-openai:latest
        args:
          - "--model"
          - "/models/hf/gemma-3-1b-it"
          - "--max-model-len"
          - "4096"
        resources:
          limits:
            nvidia.com/gpu: "1"
        ports:
        - containerPort: 8000
        volumeMounts:
        - name: model-cache-volume
          mountPath: /models
          readOnly: true
      volumes:
      - name: model-cache-volume
        persistentVolumeClaim:
          claimName: vllm-model-cache
          readOnly: true
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-l4
