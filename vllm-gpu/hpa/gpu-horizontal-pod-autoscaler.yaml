# ---
# Purpose: This HorizontalPodAutoscaler (HPA) automatically scales the vLLM
#          inference server deployment based on NVIDIA GPU utilization. It ensures
#          that expensive GPU resources are scaled up to meet demand and scaled
#          down when idle.
# ---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gemma-server-gpu-hpa
spec:
  # scaleTargetRef points the HPA to the deployment it needs to scale.
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-gemma-deployment # The name of the vLLM deployment
  # minReplicas and maxReplicas define the boundaries for scaling.
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Pods # This metric is calculated on a per-pod basis.
    pods:
      metric:
        # This is the full name of the GPU metric as exposed by the
        # Stackdriver adapter. The `dcgm_fi_dev_gpu_util` metric is created
        # by the Prometheus rule in `gpu-rules.yaml`.
        # Format: prometheus.googleapis.com|<METRIC_NAME>|<METRIC_KIND>
        name: prometheus.googleapis.com|dcgm_fi_dev_gpu_util|gauge
      target:
        type: AverageValue
        # The HPA will trigger a scale-up event if the average GPU utilization
        # across all pods exceeds this percentage (e.g., 20%).
        averageValue: 20
  behavior:
    # The scaleDown behavior helps prevent "flapping" (rapidly scaling up and down).
    scaleDown:
      # stabilizationWindowSeconds tells the HPA to wait for this duration
      # after a scale-down event before considering another one. This gives
      # the system time to stabilize.
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        # This policy allows the HPA to scale down by 100% of the current
        # replica count in a single step, if necessary.
        value: 100
        periodSeconds: 15
